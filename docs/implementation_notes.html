<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <meta name="author" content="Burak Himmetoglu" />
  <title>Implementation Notes</title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
</head>
<body>
<div id="header">
<h1 class="title">Implementation Notes</h1>
<h2 class="author">Burak Himmetoglu</h2>
<h3 class="date">January 5, 2019</h3>
</div>
<h2 id="notation">Notation</h2>
<p>The data is represented by the design matrix (a.k.a data matrix) <span class="math inline">\({\bf X} \in {\rm I\!R}^{N \times p}\)</span> where <span class="math inline">\(N\)</span> is the number of observations and <span class="math inline">\(p\)</span> is the number of features. For simplicity, we assume that the target vector <span class="math inline">\({\bf y} \in {\rm I\!R}^N\)</span> represents a learning problem in the regression setting. All the the dicsussions below can be applied to the classification setting with appropriate modifications.</p>
<p>For stacking, the data is split over k-folds: <span class="math inline">\(F_1, F_2, \dots, F_k\)</span>. Each <span class="math inline">\(F_j\)</span> represent the indices of the observations in the <span class="math inline">\(j^{th}\)</span> fold. Namely,</p>
<p><span class="math display">\[ X_{F_j} = X[F_j,:] \]</span></p>
<p>in a numpy-like notation. Negative indices <span class="math inline">\(-F_j\)</span> represent all the folds expect the <span class="math inline">\(j^{th}\)</span>.</p>
<p>During fitting/training, the model parameters are optmized. We denote the folds over which the parameters are optimized in subscripts. For example</p>
<p><span class="math display">\[ {\bf W}_{-F_i} \]</span></p>
<p>denotes that the weights <span class="math inline">\({\bf W}\)</span> are optimized over all the folds expect the <span class="math inline">\(i^{th}\)</span> fold (this would be the case when the regressor is a linear function of the data, i.e. <span class="math inline">\(f({\bf X}) = {\bf W} \cdot {\bf X}\)</span>).</p>
<p>The figure below shows a single step in stacking: The fold <span class="math inline">\(F_1\)</span> is kept as hold-out, and the folds <span class="math inline">\(F_3,F_4,F_5\)</span> are used for the fit. Predictions on the fold <span class="math inline">\(F_2\)</span> are obtained from the fitted model.</p>
<center>
<img src="img/Train1Step.jpeg" alt="Stacks" style="width:25.0%" />
</center>
<h2 id="stacking-a-single-model-over-k-folds">Stacking a single model over k-folds</h2>
<p>For illustration, we consider a case of 5-folds stacking procedure. There are 5 out-of-sample (OOS) predictions that are to be stacked, where each fold is once kept as hold-out (HO). In the illustrative linear regression model, the target vector <span class="math inline">\({\bf y}\)</span> is predicted by the linear function</p>
<p><span class="math display">\[ f(X) = {\bf X} \cdot  {\bf W} + {\bf b}\]</span></p>
<p>where <span class="math inline">\({\bf W} \in {\rm I\!R}^p\)</span> and <span class="math inline">\({\bf b} \in {\rm I\!R}\)</span>. The optimal values of <span class="math inline">\({\bf W}, \, {\bf b}\)</span> are obtained by minimizing the loss function:</p>
<p><span class="math display">\[ L({\bf W}, {\bf b}; \lambda) = ({\bf X} \cdot  {\bf W} + {\bf b} - {\bf y})^2 +  \lambda \vert {\bf W} \vert^2 \qquad , \qquad
   {\bf W}, {\bf b} \leftarrow {\rm argmin}_{W,b}\, L({\bf W}, {\bf b}; \lambda)
\]</span></p>
<p>where <span class="math inline">\(\lambda\)</span> is the hyper-parameter for the regularization term. In the initial implementation of stacking, we assume <span class="math inline">\(\lambda\)</span> to be a fixed parameter. 5 fits for <span class="math inline">\({\bf W}, {\bf b}\)</span> are performed by minimizing the loss function across folds and OOS predictions are collected. For instance, the OOS predictions for the splits pictured above are obtained by</p>
<p><span class="math display">\[ {\hat y}^{OOS}_{-F_1} = 
\begin{bmatrix}
{\hat y}_{F_2} \\
{\hat y}_{F_3} \\
{\hat y}_{F_4} \\
{\hat y}_{F_5}
\end{bmatrix}
\]</span></p>
<p>where each <span class="math inline">\({\hat y}_{F_j}\)</span> is a column vector whose length is equal to the number of observations in the <span class="math inline">\(j^{th}\)</span> fold. In the above equation, <span class="math inline">\({\hat y}_{F_2}\)</span> is obtained by optimizing the loss function on the data <span class="math inline">\(X_{-(F_1+F_2)}\)</span>, which results in <span class="math inline">\({\bf W}_{-(F_1+F_2)}, {\bf b}_{-(F_1+F_2)}\)</span> and predicting on the fold <span class="math inline">\(F_2\)</span>. Namely,</p>
<p><span class="math display">\[ {\hat y}_{F_2} = {\bf X}_{F_2} \cdot {\bf W}_{(-F_1+F_2)} + {\bf b}_{-(F_1+F_2)} \]</span></p>
<p>The rest of the predictions <span class="math inline">\({\hat y}_3, {\hat y}_4, {\hat y}_5\)</span> are obtained following a similar set of computations. The predictions on the HO fold are instead obtained by simply fitting the model on all the four folds, and predicting on the HO fold. Namely,</p>
<p><span class="math display">\[ {\hat y}^{HO}_{F_1} = {\bf X}_{F_1} \cdot {\bf W}_{-F_1} + {\bf b}_{-F_1} \]</span></p>
<p>Since the HO fold can be any of the 5 folds, we have the following set of OOS-HO predictions:</p>
<p><span class="math display">\[ \left[ ({\hat y}^{OOS}_{-F_1}, {\hat y}_{F_1}^{HO}), \dots, ({\hat y}^{OOS}_{-F_5}, {\hat y}_{F_5}^{HO}) \right] \]</span></p>
<p>As will be shown below, this set of OOS and HO predictions provide an automatic way to train the stacker models by 5-folds cross-validation.</p>
<h2 id="stacking-multiple-models-over-k-folds">Stacking multiple models over k folds</h2>
<p>Suppose we have <span class="math inline">\(M\)</span> models that we would like to stack. For each model (assuming fixed hyper-parameters), we will repeat the above procedure to get OOS and HO predictions and construct new data matrices by column-stacking. For example, when <span class="math inline">\(F_1\)</span> is the HO fold, we have</p>
<p><span class="math display">\[ {\bf X}^{OOS}_{-F_1} = 
\begin{bmatrix}
\vert &amp; \dots &amp; \vert \\
{\hat y}^{(1)\, OOS}_{-F_1} &amp; \dots &amp; {\hat y}^{(M)\, OOS}_{-F_1} \\ 
\vert &amp; \dots &amp; \vert 
\end{bmatrix}
\qquad, \qquad
{\bf X}^{HO}_{F_1} =
\begin{bmatrix}
\vert &amp; \dots &amp; \vert \\
{\hat y}_{F_1}^{(1)\, HO} &amp; \dots &amp; {\hat y}_{F_1}^{(M)\, HO} \\
\vert &amp; \dots &amp; \vert 
\end{bmatrix}
\]</span></p>
<p>Peforming the same for other HO folds, we end up with the following set of 5 pairs:</p>
<p><span class="math display">\[ \left[ ( {\bf X}^{OOS}_{-F_1}, {\bf X}_{F_1}^{HO}), \dots, ({\bf X}^{OOS}_{-F_5}, {\bf X}_{F_5}^{HO}) \right] \]</span></p>
<p>which can be used to train the stacker models by optimizing their hyper-parameters by 5-folds cross validation. The whole procedure of obtaining training/validation folds from OOS/HO pairs is decpicted in the figure below</p>
<center>
<img src="img/TrainStacks.jpeg" alt="Train" style="width:50.0%" />
</center>
<p>Once the hyper-parameters of the stacker model are determined, the final predictions can be obtained. For the sake of simplicity, let's assume that the stacker is also a linear model. The stacker model uses the OOS predictions to train and predict on the HO folds. The predictions on the first HO fold <span class="math inline">\(F_1\)</span> are obtained by</p>
<p><span class="math display">\[ {\hat Y}_{F_1} = X^{HO}_{F_1} \cdot \Omega_{-F_1}+ \beta_{-F_1} \]</span></p>
<p>where <span class="math inline">\(\Omega\)</span> and <span class="math inline">\(\beta\)</span> are parameters of the stacker model optimized during fitting:</p>
<p><span class="math display">\[ \Omega_{-F_1}, \beta_{-F_1} \leftarrow\, {\rm argmin}_{\Omega, \beta}\, 
    \left[ ( ({\bf X}^{OOS}_{-F_1} \cdot \Omega + \beta) - {\bf y}_{-F_1})^2 + \Lambda\, \vert \Omega \vert^2 \right]
\]</span></p>
<p>After performing the same set of computations for other HO folds and a grid of values for <span class="math inline">\(\Lambda\)</span>, we can train the stacker model. <span class="math inline">\(\Lambda\)</span> is determined across 5-folds by minimizing the mean-squared-error (MSE) between <span class="math inline">\({\hat y}_{F_j}\)</span> and <span class="math inline">\({\bf y}_{F_j}\)</span> for <span class="math inline">\(j=1,\dots,5\)</span>. More precisely, the value of <span class="math inline">\(\Lambda\)</span> is the one that results in the lowest average MSE across folds:</p>
<p><span class="math display">\[ \Lambda \leftarrow \, {\rm argmin}_{\Lambda} \left[ \sum_{i=1}^5\, ( {\bf y}_{F_i} - {\hat Y}_{F_i} )^2 \right]\]</span></p>
<p>Here <span class="math inline">\({\hat Y}_{F_i}\)</span> has an implicit dependence on <span class="math inline">\(\Lambda\)</span> by the above equations.</p>
<h2 id="final-fit-on-training-data">Final fit on training data</h2>
<p>After the stacker model is trained (i.e. <span class="math inline">\(\Lambda\)</span> is determined), the final fit is obtained by using all the 5 folds in <span class="math inline">\(X^{OOS}\)</span> instead of leaving one folds for HO. Namely,</p>
<p><span class="math display">\[ {\hat y}^{OOS} =
\begin{bmatrix}
{\hat y}_{F_1} \\
{\hat y}_{F_2} \\
{\hat y}_{F_3} \\
{\hat y}_{F_4} \\
{\hat y}_{F_5}
\end{bmatrix}
\]</span></p>
<p>where <span class="math inline">\({\hat y}_{F_i}\)</span> is determined as follows:</p>
<p><span class="math display">\[ {\hat y}_{F_i} = {\bf X}_{F_i} \cdot {\bf W}_{-F_i} + {\bf b}_{-F_i} \qquad , \qquad
   {\bf W}_{-F_i}, {\bf b}_{-F_i} \leftarrow {\rm argmin}_{W,b} \left[ ({\bf X}_{-F_i} \cdot {\bf W}+ {\bf b} - {\bf y}_{-F_i})^2
      + \lambda \vert {\bf W} \vert^2 \right]
\]</span></p>
<p>Column-stacking theese column vectors for M models results in <span class="math inline">\({\bf X}^{OOS}\)</span>. With <span class="math inline">\(\Lambda\)</span> determined by the 5-fold cross validation procedure from above, the final predictions are provided by the stacker model:</p>
<p><span class="math display">\[ {\hat Y} = {\bf X}^{OOS} \cdot {\Omega} + \beta \qquad, \qquad
   \Omega, \beta \leftarrow {\rm argmin}_{\Omega, \beta} \ \left[ ({\bf X}^{OOS} \cdot \Omega + \beta - {\bf y})^2
      + \Lambda \vert \Omega \vert^2 \right]
\]</span></p>
<h2 id="final-fit-on-test-data">Final fit on test data</h2>
<p>The test data contains a completely new set of observations that need to go through the stacking procedure as well. In the first step, we also need to split the test data on 5-folds and predict using the 5 different sets of parameters <span class="math inline">\({\bf W}_{-F_i}, {\bf b}_{-F_i}\)</span>. Unlike the training set where it is clear which fold is predicted by which model parameters (i.e. <span class="math inline">\(F_1\)</span> is predicted by the model that uses <span class="math inline">\({\bf W}_{-F_1}, {\bf b}_{-F_1}\)</span>), for the test set there is no natural one-to-one correspondance. We therefore predict on each test fold by each model, resulting in a <span class="math inline">\(5 \times 5\)</span> set of predictions for <strong>each model!</strong></p>
<p>Let's assume that we are stacking a single model's predictions. We construct the following matrix for this purpose:</p>
<p><span class="math display">\[
  {\bf Z}^{(1)} = 
\begin{bmatrix}
 {\bf X}^{\rm test}_{F_1} \cdot {\bf W}_{-F_1} + {\bf b}_{-F_1} &amp; {\bf X}^{\rm test}_{F_1} \cdot {\bf W}_{-F_2} + {\bf b}_{-F_2} &amp; \dots &amp;
 {\bf X}^{\rm test}_{F_1} \cdot {\bf W}_{-F_5} + {\bf b}_{-F_5} \\
 {\bf X}^{\rm test}_{F_2} \cdot {\bf W}_{-F_1} + {\bf b}_{-F_1} &amp; {\bf X}^{\rm test}_{F_2} \cdot {\bf W}_{-F_2} + {\bf b}_{-F_2} &amp; \dots &amp;
{\bf X}^{\rm test}_{F_2} \cdot {\bf W}_{-F_5} + {\bf b}_{-F_5} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
{\bf X}^{\rm test}_{F_5} \cdot {\bf W}_{-F_1} + {\bf b}_{-F_1} &amp; {\bf X}^{\rm test}_{F_5} \cdot {\bf W}_{-F_2} + {\bf b}_{-F_2} &amp; \dots &amp;
{\bf X}^{\rm test}_{F_5} \cdot {\bf W}_{-F_5} + {\bf b}_{-F_5}
\end{bmatrix}
\]</span></p>
<p>We need one column vector from <span class="math inline">\({\bf Z}^{(1)}\)</span>, which we choose to pick by taking the row averages:</p>
<p><span class="math display">\[
  {\hat y}^{\rm test, OOS} = \frac{1}{5}\, 
\begin{bmatrix}
\sum_{j}\, Z^{(1)}_{1j} \\
\vdots \\
\sum_{j}\, Z^{(1)}_{5j}
\end{bmatrix}
\]</span></p>
<p>Column-stacking the test OOS predictions into <span class="math inline">\({\bf X}^{\rm test, OOS}\)</span> for <span class="math inline">\(M\)</span> models, we can then predict the test data using the stacker model (with <span class="math inline">\(\Omega, \beta\)</span> determined from the training set).</p>
<h2 id="training-models-to-be-stacked-in-addition-to-stacker">Training models to be stacked in addition to stacker</h2>
<p>Up to this point we have assumed that the hyper-parameters (<span class="math inline">\(\lambda\)</span>) of all the initial models are fixed. This led to 5 models characterized by parameters <span class="math inline">\({\bf W}_{-F_i}, {\bf b}_{-F_i}\)</span>, for <span class="math inline">\(i=1,\dots,5\)</span>. Instead, for each HO fold an internal 4-fold cross-validation can be performed to optimize the hyper-parameters of each input model during training. While this procedure leads to an increased computational time, it usually results in better generalizability. This results in 5 fits per model with differing hyper-parameters.</p>
</body>
</html>
